1. Prerequisites
Java is the primary requirement for running Hadoop on any system, So make sure you have
Java installed on your system using the following command. If you don’t have Java installed on
your system, use one of the following links to install it first. Hadoop supports only JAVA 8 If
already any other version is present then uninstall the following using thesecommands.
sudo apt-get purge openjdk-\* icedtea-\* icedtea6-\*
OR
sudo apt remove openjdk-8-jdk
● Step 1.1 – Install Oracle Java 8 onUbuntu
You need to enable additional repository to your system to install Java 8 on Ubuntu VPS.After
that install Oracle Java 8 on an Ubuntu system using apt-get.This repository contains apackage
named oracle-java8-installer, Which is notan actual Java package. Instead of that, this package
contains a script toinstall Java on Ubuntu.Run below commands to install Java 8 on Ubuntu
andLinuxMint.
sudo add-apt-repository
ppa:webupd8team/java sudo apt-get
sudo apt-get installoracle-java8-installer
OR
sudo apt install openjdk-8-jre-headless
sudo apt install openjdk-8-jdk
● Step 1.2 – Verify JavaInatallation
The apt repository also provides package oracle-java8-set-default to set Java 8 as your default
Java version. This package will be installed along with Java installation. To make sure run
below command.
sudo apt-get install oracle-java8-set-default
After successfully installing Oracle Java 8 using the above steps, Let’s verify the installed version using the following command.
java -version
java version "1.8.0_201" 
Java(TM) SE Runtime Environment (build 1.8.0_201-b09)
Java HotSpot(TM) 64-Bit Server VM (build 25.201-b09, mixed mode)
● Step 1.3 – Setup JAVA_HOME and JRE_HOMEVariable
Add the java path to JAVA_HOME variable in .bashrc file. Go to your home directory and in the
folder option click on show hidden files. After that a .bashrc file will be present, open thefile
and add the following line at theend.
NOTE- Path of the java will be your pc path on which java is been
installed. export JAVA_HOME=/usr/lib/jvm/java-8-oracle
NOTE- After doing all changes and saving the file run the following command to make
changes through the .bashrc file.
source ~/.bashrc
All done, you have successfully installed Java 8 on a Linux system.
2. Create Hadoop User
We recommend creating a normal (nor root) account for Hadoop working. To create
an account using the following command.
Add user hadoop passwd hadoop
Set up a new user for Hadoop working separately other than the normal users.
NOTE- Its compulsory to create a sperate user with username hadoop otherwise it may
give you path file issues later.
prequsit
********
linux operating system(ubuntu 16)
internet
java
ssh
hadoop package
************
hadoop --- three mode
standalone mode--debbugging mode
pseduo distributed mode --- development enviroment
fully distributed mode -- production enviroment
********
standalone mode
step 1
********
sudo apt-get update
java -version
sudo apt-get install default-jdk
java -version
*********
step 2
sudo apt-get install ssh
ssh-keygen
ls -a
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
ssh localhost
*************
step 3
https://www-eu.apache.org/dist/hadoop/common/
wget https://www-eu.apache.org/dist/hadoop/common/hadoop-2.8.4/hadoop-2.8.4.tar.gz
tar -xvf hadoop-2.8.4.tar.gz
mv hadoop-2.8.4 hadoop
************
step 4
enviroment setup in linux for hadoop
vim .bashrc
.
.
..
fi
fi
#HADOOP VARIABLES START
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export HADOOP_HOME=/home/ubuntu/hadoop
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib"
#HADOOP VARIABLES END
:wq
source .bashrc
hadoop version
***********************
pseduodistrubuted mode
step-5
cd hadoop/etc/hadoop/
ls
1. hadoop-env.sh
2. hdfs-site.xml
<property>
<name>dfs.replication</name>
<value>1</value>
</property>
3. core-site.xml
<property>
<name>fs.defaultFS</name>
<value>hdfs://localhost:9000</value>
</property>
<property>
<name>hadoop.tmp.dir</name>
<value>/home/ubuntu/hdata</value>
</property>
4.mv mapred-site.xml.template mapred-site.xml
mapred-site.xml
<property>
<name>mapreduce.framework.name</name>
<value>yarn</value>
</property>
5.vim yarn-site.xml
<property>
<name>yarn.nodemanager.aux-services</name>
<value>mapreduce_shuffle</value>
</property>
<property>
<name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
<value>org.apache.hadoop.mapred.ShuffleHandler</value>
</property>
************
step-6
hdfs namenode -format
start-all.sh
jps
3. Start HadoopCluster
Let’s start your Hadoop cluster using the scripts provides by Hadoop. Just navigate to
your $HADOOP_HOME/sbin directory and execute scripts one by one.
Cd $HADOOP_HOME/sbin/
Now run start-dfs.sh script.
./start-dfs.sh
Sample output:Starting namenodes on
[localhost] Starting datanodes
Starting secondary namenodes [localhost]
2018-05-02 18:00:32,565 WARN util.NativeCodeLoader: Unable to load native-hadoop
library for your platform... using builtin-java classes where applicable
Now run start-yarn.sh script.
./start-yarn.sh
Sample output:Starting resourcemanager
Starting nodemanagers
4. Access Hadoop Services in Browser
Hadoop NameNode started on port 9870 default. Access your server on port 9870 in
your favorite web browser.
http://localhost:9870/
Now access port 8042 for getting the information about the cluster and all applications http:// localhost:8042/
Access port 9864 to get details about your Hadoop node.
http://localhost:9864/
5. Test Hadoop Single NodeSetup
Make the HDFS directories required using following commands.
bin/hdfs dfs -mkdir /user
bin/hdfs dfs -mkdir /user/hadoop
Copy all files from local file system /var/log/httpd to hadoop distributed
filesystem using belowcommand
bin/hdfs dfs -put /var/log/apache2logs
Browse Hadoop distributed file system by opening below URL in the browser.
You will see an apache2 folder in thelist.
http://localhost:9870/explorer.html#/user/hadoop/logs/
